{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"6TZJT4ncbYiG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This section of code extracts frames from videos and stores them in the same folder where the video is stored. At the same time, it creates a metadata named data_file.csv that stores information like name of the video file, number of frames, class name and whether it is train or test dataset."],"metadata":{"id":"cuqwREsaPDDE"}},{"cell_type":"code","metadata":{"id":"jYpf2INDzzZm"},"source":["import csv\n","import glob\n","import os.path\n","from subprocess import call\n","import os\n","from random import shuffle\n","\n","def extract_files():\n","    data_file = []\n","    folders = ['train', 'test']\n","    for folder in folders:\n","        class_folders = glob.glob(os.path.join(folder, '*'))\n","\n","        for vid_class in class_folders:\n","            class_files = glob.glob(os.path.join(vid_class, '*.mp4'))\n","            # print(class_files)\n","\n","            for video_path in class_files:\n","                # Get the parts of the file.\n","                video_parts = get_video_parts(video_path)\n","\n","                train_or_test, classname, filename_no_ext, filename = video_parts\n","\n","                # Only extract if we haven't done it yet. Otherwise, just get\n","                # the info.\n","                if not check_already_extracted(video_parts):\n","                    # Now extract it.\n","                    src = os.path.join(train_or_test, classname, filename)\n","                    dest = os.path.join(train_or_test, classname,\n","                        filename_no_ext + '-%04d.jpg')\n","                    call([\"ffmpeg\", \"-i\", src, dest])\n","\n","                # Now get how many frames it is.\n","                nb_frames = get_nb_frames_for_video(video_parts)\n","\n","                data_file.append([train_or_test, classname, filename_no_ext, nb_frames])\n","\n","                print(\"Generated %d frames for %s\" % (nb_frames, filename_no_ext))\n","    shuffle(data_file)\n","\n","    with open('data_file.csv', 'w') as fout:\n","        writer = csv.writer(fout)\n","        writer.writerows(data_file)\n","    #[train|test], class, filename, nb frames\n","    print(\"Extracted and wrote %d video files.\" % (len(data_file)))\n","\n","def get_nb_frames_for_video(video_parts):\n","    train_or_test, classname, filename_no_ext, _ = video_parts\n","    generated_files = glob.glob(os.path.join(train_or_test, classname,\n","                                filename_no_ext + '*.jpg'))\n","    return len(generated_files)\n","\n","def get_video_parts(video_path):\n","    parts = video_path.split(os.path.sep)\n","    filename = parts[2]\n","    filename_no_ext = filename.split('.')[0]\n","    classname = parts[1]\n","    train_or_test = parts[0]\n","\n","    return train_or_test, classname, filename_no_ext, filename\n","\n","def check_already_extracted(video_parts):\n","    train_or_test, classname, filename_no_ext, _ = video_parts\n","    return bool(os.path.exists(os.path.join(train_or_test, classname,\n","                               filename_no_ext + '-0001.jpg')))\n","\n","extract_files()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This section of code generates features from overall frames using transfer learning and stores them using numpy in \"data/sequences\" folder"],"metadata":{"id":"CIsFykE-QmcG"}},{"cell_type":"code","metadata":{"id":"r1akelL0pf4v"},"source":["import csv\n","import numpy as np\n","import random\n","import glob\n","import os.path\n","import sys\n","import operator\n","import threading\n","from tensorflow.keras.utils import to_categorical\n","class DataSet():\n","\n","    def __init__(self, seq_length=40, class_limit=None, image_shape=(224, 224, 3)):\n","        self.seq_length = seq_length\n","        self.class_limit = class_limit\n","        self.sequence_path = os.path.join('data', 'sequences')\n","        self.max_frames = 300  # max number of frames a video can have for us to use it\n","        self.data = self.get_data()\n","        self.classes = self.get_classes()\n","        self.data = self.clean_data()\n","        self.image_shape = image_shape\n","        os.makedirs(\"data/sequences\",exist_ok=True)\n","\n","\n","    @staticmethod\n","    def get_data():\n","        with open(os.path.join('data_file.csv'), 'r') as fin:\n","            reader = csv.reader(fin)\n","            data = list(reader)\n","        return data\n","\n","    def clean_data(self):\n","        data_clean = []\n","        for item in self.data:\n","            if int(item[3]) >= self.seq_length and int(item[3]) <= self.max_frames \\\n","                    and item[1] in self.classes:\n","                data_clean.append(item)\n","\n","        return data_clean\n","\n","    def get_classes(self):\n","        classes = []\n","        for item in self.data:\n","            if item[1] not in classes:\n","                classes.append(item[1])\n","        classes = sorted(classes)\n","        if self.class_limit is not None:\n","            return classes[:self.class_limit]\n","        else:\n","            return classes\n","\n","    def get_class_one_hot(self, class_str):\n","        # Encode it first.\n","        label_encoded = self.classes.index(class_str)\n","        # Now one-hot it.\n","        label_hot = to_categorical(label_encoded, len(self.classes))\n","        assert len(label_hot) == len(self.classes)\n","        return label_hot\n","\n","    def split_train_test(self):\n","        train = []\n","        test = []\n","        for item in self.data:\n","            if item[0] == 'train':\n","                train.append(item)\n","            else:\n","                test.append(item)\n","        return train, test\n","\n","    def get_all_sequences_in_memory(self, train_test, data_type):\n","        train, test = self.split_train_test()\n","        data = train if train_test == 'train' else test\n","\n","        print(\"Loading %d samples into memory for %sing.\" % (len(data), train_test))\n","\n","        X, y = [], []\n","        for row in data:\n","            sequence = self.get_extracted_sequence(data_type, row)\n","            if sequence is None:\n","                print(\"Can't find sequence. Did you generate them?\")\n","                raise\n","            X.append(sequence)\n","            y.append(self.get_class_one_hot(row[1]))\n","        return np.array(X), np.array(y)\n","\n","    def get_extracted_sequence(self, data_type, sample):\n","        filename = sample[2]\n","        path = os.path.join(self.sequence_path, filename + '-' + str(self.seq_length) + \\\n","            '-' + data_type + '.npy')\n","        if os.path.isfile(path):\n","            return np.load(path)\n","        else:\n","            return None\n","\n","    def get_frames_by_filename(self, filename, data_type):\n","        sample = None\n","        for row in self.data:\n","            if row[2] == filename:\n","                sample = row\n","                break\n","        if sample is None:\n","            raise ValueError(\"Couldn't find sample: %s\" % filename)\n","        sequence = self.get_extracted_sequence(data_type, sample)\n","        if sequence is None:\n","            raise ValueError(\"Can't find sequence. Did you generate them?\")\n","        return sequence\n","\n","    @staticmethod\n","    def get_frames_for_sample(sample):\n","        \"\"\"Given a sample row from the data file, get all the corresponding frame\n","        filenames.\"\"\"\n","        path = os.path.join(sample[0], sample[1])\n","        filename = sample[2]\n","        images = sorted(glob.glob(os.path.join(path, filename + '*jpg')))\n","        return images\n","\n","    @staticmethod\n","    def rescale_list(input_list, size):\n","        assert len(input_list) >= size\n","        skip = len(input_list) // size\n","        output = [input_list[i] for i in range(0, len(input_list), skip)]\n","        return output[:size]\n","\n","\n","import numpy as np\n","import os.path\n","from keras.preprocessing import image as Img\n","from keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from keras.models import Model, load_model\n","from keras.layers import Input\n","from tqdm import tqdm\n","\n","seq_length=40\n","\n","# Get the dataset.\n","data = DataSet(seq_length=40, class_limit=None)\n","\n","base_model = InceptionV3(\n","    weights='imagenet',\n","    include_top=True\n",")\n","# We'll extract features at the final pool layer.\n","model = Model(\n","    inputs=base_model.input,\n","    outputs=base_model.get_layer('avg_pool').output\n",")\n","\n","# Loop through data.\n","pbar = tqdm(total=len(data.data))\n","for video in data.data:\n","\n","    # Get the path to the sequence for this video.\n","    path = os.path.join('data', 'sequences', video[2] + '-' + str(seq_length) + \\\n","        '-features')  # numpy will auto-append .npy\n","    # Check if we already have it.\n","    if os.path.isfile(path + '.npy'):\n","        pbar.update(1)\n","        continue\n","\n","    # Get the frames for this video.\n","    frames = data.get_frames_for_sample(video)\n","    #print(frames)\n","\n","    # Now downsample to just the ones we need.\n","    frames = data.rescale_list(frames, 40)\n","    #print(frames)\n","    #extracting features and appending to build the sequence.\n","    sequence = []\n","    for image in frames:\n","        img = Img.load_img(image, target_size=(299, 299))\n","        x = Img.img_to_array(img)\n","        x = np.expand_dims(x, axis=0)\n","        x = preprocess_input(x)\n","        features = model.predict(x)\n","        sequence.append(features[0])\n","\n","    # Save the sequence.\n","    np.save(path, sequence)\n","\n","    pbar.update(1)\n","\n","pbar.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This section of code trains the model and stores the checkpoints in a folder named \"checkpoints\" in the drive."],"metadata":{"id":"07y09QnnRVC5"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"by7XIMwbuZ5o","executionInfo":{"status":"ok","timestamp":1612427476808,"user_tz":-345,"elapsed":41577,"user":{"displayName":"Sailesh Rana","photoUrl":"","userId":"08308549808075758303"}},"outputId":"7b4b45e2-083f-49d9-b0cb-caa946d27558"},"source":["from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D, BatchNormalization, Bidirectional, Activation\n","from keras.layers.recurrent import LSTM\n","from keras.models import Sequential, load_model\n","from tensorflow.keras.optimizers import Adam, SGD\n","from collections import deque\n","import matplotlib.pyplot as plt\n","import sys\n","from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n","import time\n","import os.path\n","os.makedirs(\"/content/drive/MyDrive/checkpoints\",exist_ok=True)\n","checkpoint_path = \"/content/drive/MyDrive/checkpoints/cp-{epoch:04d}.hdf5\"\n","checkpointer = ModelCheckpoint(\n","    filepath= checkpoint_path,\n","    verbose=1,\n","    save_best_only=True)\n","\n","def plot_learning_curves(history):\n","\n","    plt.plot(history.history[\"loss\"])\n","    plt.plot(history.history[\"val_loss\"])\n","    plt.title(\"Model Loss\")\n","    plt.ylabel(\"Loss\")\n","    plt.xlabel(\"Epoch\")\n","    plt.legend([\"Train\", \"Val\"], loc=\"upper right\")\n","    plt.savefig(\"Model Loss.png\")\n","    plt.show()\n","\n","Helper: TensorBoard\n","tb = TensorBoard(log_dir=os.path.join('data', 'logs', 'lstm'))\n","\n","# Helper: Stop when we stop learning.\n","early_stopper = EarlyStopping(patience=10)\n","\n","# Helper: Save results.\n","timestamp = time.time()\n","csv_logger = CSVLogger(os.path.join('data', 'logs', 'lstm' + '-' + 'training-' + \\\n","    str(timestamp) + '.log'))\n","\n","# Get the data and process it.\n","data = DataSet(\n","    seq_length=40,\n","    class_limit=None\n",")\n","print(len(data.classes))\n","#listt=[]\n","#listt2=[]\n","HIDDEN_UNITS = 512\n","X, y = data.get_all_sequences_in_memory('train', 'features')\n","\n","X_test, y_test = data.get_all_sequences_in_memory('test', 'features')\n","\n","model = Sequential()\n","\n","model.add(LSTM(256,return_sequences=True, input_shape=(40,2048), dropout=0.4))\n","model.add(BatchNormalization())\n","# model.add(Dropout(0.6))\n","model.add(LSTM(128))\n","model.add(Dense(16, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(data.classes), activation='softmax'))\n","optimizer = Adam(lr=1e-5, decay=1e-6)\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n","                    metrics=['accuracy','top_k_categorical_accuracy'])\n","print(model.summary())\n","\n","history = model.fit(\n","    X,\n","    y,\n","    batch_size=32,\n","    validation_data=(X_test, y_test),\n","    verbose=1,\n","    shuffle=True,\n","    callbacks=[tb, early_stopper, csv_logger, checkpointer],\n","    epochs=100)\n","\n","plot_learning_curves(history)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5\n","Loading 1640 samples into memory for training.\n","Loading 169 samples into memory for testing.\n","Model: \"sequential_24\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_46 (LSTM)               (None, 40, 256)           2360320   \n","_________________________________________________________________\n","batch_normalization_24 (Batc (None, 40, 256)           1024      \n","_________________________________________________________________\n","lstm_47 (LSTM)               (None, 128)               197120    \n","_________________________________________________________________\n","dense_46 (Dense)             (None, 16)                2064      \n","_________________________________________________________________\n","dropout_24 (Dropout)         (None, 16)                0         \n","_________________________________________________________________\n","dense_47 (Dense)             (None, 5)                 85        \n","=================================================================\n","Total params: 2,560,613\n","Trainable params: 2,560,101\n","Non-trainable params: 512\n","_________________________________________________________________\n","None\n","Epoch 1/100\n","52/52 [==============================] - 4s 37ms/step - loss: 1.6811 - accuracy: 0.2442 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.5801 - val_accuracy: 0.2722 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00001: val_loss improved from inf to 1.58013, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0001.hdf5\n","Epoch 2/100\n","52/52 [==============================] - 1s 19ms/step - loss: 1.4265 - accuracy: 0.3744 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.5374 - val_accuracy: 0.3314 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00002: val_loss improved from 1.58013 to 1.53740, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0002.hdf5\n","Epoch 3/100\n","52/52 [==============================] - 1s 18ms/step - loss: 1.2817 - accuracy: 0.4555 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.4961 - val_accuracy: 0.3905 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00003: val_loss improved from 1.53740 to 1.49613, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0003.hdf5\n","Epoch 4/100\n","52/52 [==============================] - 1s 18ms/step - loss: 1.1418 - accuracy: 0.5641 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.4566 - val_accuracy: 0.4793 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00004: val_loss improved from 1.49613 to 1.45660, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0004.hdf5\n","Epoch 5/100\n","52/52 [==============================] - 1s 18ms/step - loss: 1.0337 - accuracy: 0.6306 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.4286 - val_accuracy: 0.4911 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00005: val_loss improved from 1.45660 to 1.42862, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0005.hdf5\n","Epoch 6/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.9759 - accuracy: 0.6554 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.4031 - val_accuracy: 0.5266 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00006: val_loss improved from 1.42862 to 1.40314, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0006.hdf5\n","Epoch 7/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.8673 - accuracy: 0.7077 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3903 - val_accuracy: 0.5621 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00007: val_loss improved from 1.40314 to 1.39034, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0007.hdf5\n","Epoch 8/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.8324 - accuracy: 0.7238 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3590 - val_accuracy: 0.5799 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00008: val_loss improved from 1.39034 to 1.35896, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0008.hdf5\n","Epoch 9/100\n","52/52 [==============================] - 1s 19ms/step - loss: 0.7864 - accuracy: 0.7403 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3609 - val_accuracy: 0.5858 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00009: val_loss did not improve from 1.35896\n","Epoch 10/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.7211 - accuracy: 0.7596 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3451 - val_accuracy: 0.5799 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00010: val_loss improved from 1.35896 to 1.34514, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0010.hdf5\n","Epoch 11/100\n","52/52 [==============================] - 1s 25ms/step - loss: 0.7066 - accuracy: 0.7706 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3402 - val_accuracy: 0.5621 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00011: val_loss improved from 1.34514 to 1.34019, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0011.hdf5\n","Epoch 12/100\n","52/52 [==============================] - 1s 22ms/step - loss: 0.6371 - accuracy: 0.8072 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3148 - val_accuracy: 0.5562 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00012: val_loss improved from 1.34019 to 1.31475, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0012.hdf5\n","Epoch 13/100\n","52/52 [==============================] - 1s 23ms/step - loss: 0.6192 - accuracy: 0.8048 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3011 - val_accuracy: 0.5858 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00013: val_loss improved from 1.31475 to 1.30111, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0013.hdf5\n","Epoch 14/100\n","52/52 [==============================] - 1s 19ms/step - loss: 0.5945 - accuracy: 0.8214 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2865 - val_accuracy: 0.5740 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00014: val_loss improved from 1.30111 to 1.28653, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0014.hdf5\n","Epoch 15/100\n","52/52 [==============================] - 1s 19ms/step - loss: 0.5585 - accuracy: 0.8166 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2812 - val_accuracy: 0.5680 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00015: val_loss improved from 1.28653 to 1.28116, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0015.hdf5\n","Epoch 16/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.5596 - accuracy: 0.8051 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2860 - val_accuracy: 0.5680 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00016: val_loss did not improve from 1.28116\n","Epoch 17/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.5332 - accuracy: 0.8180 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2855 - val_accuracy: 0.5680 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00017: val_loss did not improve from 1.28116\n","Epoch 18/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.4943 - accuracy: 0.8402 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2779 - val_accuracy: 0.5740 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00018: val_loss improved from 1.28116 to 1.27794, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0018.hdf5\n","Epoch 19/100\n","52/52 [==============================] - 1s 22ms/step - loss: 0.4801 - accuracy: 0.8396 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2833 - val_accuracy: 0.5562 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00019: val_loss did not improve from 1.27794\n","Epoch 20/100\n","52/52 [==============================] - 1s 23ms/step - loss: 0.4551 - accuracy: 0.8570 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2810 - val_accuracy: 0.5740 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00020: val_loss did not improve from 1.27794\n","Epoch 21/100\n","52/52 [==============================] - 1s 19ms/step - loss: 0.4416 - accuracy: 0.8557 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2586 - val_accuracy: 0.5799 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00021: val_loss improved from 1.27794 to 1.25860, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0021.hdf5\n","Epoch 22/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.4278 - accuracy: 0.8664 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2582 - val_accuracy: 0.5858 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00022: val_loss improved from 1.25860 to 1.25821, saving model to /content/drive/MyDrive/Video Sequence Classification/checkpoints/cp-0022.hdf5\n","Epoch 23/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.4347 - accuracy: 0.8606 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2861 - val_accuracy: 0.5799 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00023: val_loss did not improve from 1.25821\n","Epoch 24/100\n","52/52 [==============================] - 1s 20ms/step - loss: 0.4380 - accuracy: 0.8631 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2684 - val_accuracy: 0.5799 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00024: val_loss did not improve from 1.25821\n","Epoch 25/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.4091 - accuracy: 0.8721 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.2994 - val_accuracy: 0.5621 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00025: val_loss did not improve from 1.25821\n","Epoch 26/100\n","52/52 [==============================] - 1s 19ms/step - loss: 0.4104 - accuracy: 0.8522 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3144 - val_accuracy: 0.5680 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00026: val_loss did not improve from 1.25821\n","Epoch 27/100\n","52/52 [==============================] - 1s 20ms/step - loss: 0.3976 - accuracy: 0.8632 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3354 - val_accuracy: 0.5621 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00027: val_loss did not improve from 1.25821\n","Epoch 28/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.3621 - accuracy: 0.8754 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3206 - val_accuracy: 0.5680 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00028: val_loss did not improve from 1.25821\n","Epoch 29/100\n","52/52 [==============================] - 1s 20ms/step - loss: 0.4089 - accuracy: 0.8565 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3228 - val_accuracy: 0.5917 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00029: val_loss did not improve from 1.25821\n","Epoch 30/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.3327 - accuracy: 0.8883 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3036 - val_accuracy: 0.5917 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00030: val_loss did not improve from 1.25821\n","Epoch 31/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.3771 - accuracy: 0.8686 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3056 - val_accuracy: 0.5799 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00031: val_loss did not improve from 1.25821\n","Epoch 32/100\n","52/52 [==============================] - 1s 18ms/step - loss: 0.3273 - accuracy: 0.8940 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.3201 - val_accuracy: 0.5976 - val_top_k_categorical_accuracy: 1.0000\n","\n","Epoch 00032: val_loss did not improve from 1.25821\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","source":["This section of code tests the trained model on a single video file"],"metadata":{"id":"lsePG1RDRqhd"}},{"cell_type":"code","metadata":{"id":"YE3UFlQK6e0p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612428347362,"user_tz":-345,"elapsed":9004,"user":{"displayName":"Sailesh Rana","photoUrl":"","userId":"08308549808075758303"}},"outputId":"79dc308d-3679-4b4e-854d-4c7f31ec9106"},"source":["from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n","from keras.layers.recurrent import LSTM\n","from keras.models import Sequential, load_model\n","from keras.optimizers import Adam\n","from collections import deque\n","import sys\n","from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n","import time\n","import os.path\n","\n","model = Sequential()\n","model.add(LSTM(256,return_sequences=True, input_shape=(40,2048), dropout=0.4))\n","model.add(BatchNormalization())\n","# model.add(Dropout(0.6))\n","model.add(LSTM(128))\n","model.add(Dense(16, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(data.classes), activation='softmax'))\n","model.load_weights('/content/cp-0021.hdf5')\n","\n","\n","import numpy as np\n","import os.path\n","from keras.preprocessing import image as Img\n","from keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from keras.models import Model, load_model\n","from keras.layers import Input\n","import glob\n","\n","def rescale_list(input_list, size):\n","    assert len(input_list) >= size\n","    skip = len(input_list) // size\n","    output = [input_list[i] for i in range(0, len(input_list), skip)]\n","    return output[:size]\n","classes = [\"father\", \"food\", \"promise\", \"tea\", \"wife\"]\n","import cv2 \n","import os \n","image_name = 'promise3.mp4'\n","cam = cv2.VideoCapture(image_name) \n","currentframe = 0\n","frames = []\n","\n","while(True): \n","    ret,frame = cam.read() \n","    if ret: \n","        # if video is still left continue creating images \n","        name = 'testFinal/frame'+image_name +\"frame_no\"+ str(currentframe) + '.jpg'\n","        cv2.imwrite(name, frame) \n","        frames.append(name)  \n","        currentframe += 1\n","    else: \n","        break\n","cam.release() \n","cv2.destroyAllWindows() \n","rescaled_list = rescale_list(frames,40)\n","\n","base_model = InceptionV3(\n","    weights='imagenet',\n","    include_top=True\n",")\n","# We'll extract features at the final pool layer.\n","inception_model = Model(\n","    inputs=base_model.input,\n","    outputs=base_model.get_layer('avg_pool').output\n",")\n","sequence = []\n","for image in rescaled_list:\n","        img = Img.load_img(image, target_size=(299, 299))\n","        x = Img.img_to_array(img)\n","        x = np.expand_dims(x, axis=0)\n","        x = preprocess_input(x)\n","        features = inception_model.predict(x)\n","        sequence.append(features[0])\n","\n","sequence = np.array([sequence])\n","prediction = model.predict(sequence)\n","maxm = prediction[0][0]\n","maxid = 0\n","for i in range(len(prediction[0])):\n","  if(maxm<prediction[0][i]):\n","    maxm = prediction[0][i]\n","    maxid = i\n","\n","\n","print(image_name,' ------- ',classes[maxid])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["promise3.mp4  -------  father\n"],"name":"stdout"}]}]}